---
title: "miRNA analysis  on breast cancer data"
author: "Emmanuel Dumont, PhD"
output: rmarkdown::github_document

---

## Introduction to the problem

Like many areas in genomics (the study of the genome sequence) and transcriptomics (the study of the genome expression), we look at a high-dimensionality problem where the number of features is much larger than the number of samples in the dataset. 

In this particular case, our dataset comes from the expression count of micro-RNAs ("miRNAs") that were extracted from formalin-fixed paraffin-embedded tissue specimens. All tissue specimens come from women's breast diagnosed with cancer. "Control" tissues have non-metastatic breast cancer while "Case" tissues have metastatic breast cancer. Tissues were paired in (Case, Control) when the cancers had the same tumor grade and came from women with the age. miRNAs are single-stranded non-coding RNA with about two dozen nucleotides.

The dataset comprises of 290 samples (half comes from healthy women and half come women diagnosed with breast cancer) and ~635k miRNAs were profiled. The goal of this project is to find a set of miRNAs whose expression levels can be used to differentiate between healthy patients and patients with breast cancer.

The work is divided in two steps:
1. Clustering the miRNAs 
2. Apply machine-learning models on representatives of the clusters.



```{r, initialization, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#knitr::opts_knit$set(root.dir = "/Users/Emmanuel/code/mirna-breast-cancer/")

```


```{R libraries, include = FALSE }
library(knitr)

# For DESeq2
library(DESeq2)
library(IHW)
library(data.table)

library(matrixStats) # for rowMax

# Libraries for MDS
library(magrittr)
library(dplyr)
library(ggpubr)

# Libraries for ML
library(tidyverse)
library(caret)
library(glmnet)
library(ROCR)
library(e1071) # svm
library(rpart) # regression decision trees

```


## Import, clean the data, and filter the data based on coverage

### Import the matrices of miRNA expression counts per sample

In this file, the NAs were replaced with zeros.

After importing, we concatenate the first two columns (mir_id and mir_seq) to have a unique identifier per miRNA. We then rename  each row with the miRNA new name.



```{R import-file}

# Raw counts where NAs were replaced with zeros
rawCounts <- read.csv(file = 'raw_data/miRNa_count_noNA.txt',
                       sep = '\t',header = TRUE)

# Head of the matrix of counts
head(rawCounts[1:3,1:4])

# minimize all caps (better when using the package DESEq2)
colnames(rawCounts) = tolower(colnames(rawCounts))

# Obtain the sample names (The first two columns are the miRNA ID and its sequence)
sampleNames <- colnames(rawCounts)[3:ncol(rawCounts)]

# Check that every sample is named uniquely
if (length(unique(sampleNames)) != length(sampleNames)){
  print("CHECK. There are duplicate names")
}

# Concatenate the first two columns of the miR dataframe to create 
# a unique ID per isoform
rawCounts$mir_rna <- paste(rawCounts$mir_id, "_", rawCounts$mir_seq, sep = "")

# Re-construct a new set of columns
rawCounts = rawCounts[, c('mir_rna', sampleNames)]

# Remove the "_count" from the samples' names
names(rawCounts) <- gsub("_count", "", names(rawCounts))

# Use the first column as row names
rownames(rawCounts) <- rawCounts$mir_rna
rawCounts <- rawCounts[,-1]

# Delete variables.
rm(sampleNames)

# Head of the matrix of counts
head(rawCounts[1:3,1:4])

nbSamplesRaw <- ncol(rawCounts)
nbmiRNAsRaw <- nrow(rawCounts)


```



### Import the sample info file

In the matrix above, we do not know if a sample is "Control" (healthy) or "Case" (Breast Cancer). The sample info file will give us this information. 

There are 137 "Case" samples and 125 "Control" samples.


```{R sample-info}

# We import the file created by CD who flagged the samples to remove from the analysis and flagged the samples as "Case" or "Control"
sampleInfo <- read.csv(file = 'raw_data/sample_info_DC.csv', header = TRUE)

# Remove the samples identified to be excluded (37 samples)
sampleInfo = sampleInfo[is.na(sampleInfo$exclude), ]

# Remove the samples that neither case or control (71 samples)
sampleInfo = sampleInfo[sampleInfo$status %in%c('Case', 'Control'), ]

# Keep the columns of interest
sampleInfo = sampleInfo[,c('subsample','status')]

# Rename the columns
colnames(sampleInfo) = c('sample','condition')

# Remove the duplicate samples
sampleInfo =  sampleInfo[!duplicated(sampleInfo$sample),]

# Remove the rows where the condition is neither 'Case' or 'Control'
sampleInfo = subset(sampleInfo, condition=="Case" | condition=="Control")

# Lower case
sampleInfo$sample = tolower(sampleInfo$sample)

# Rename samples so that "OL_sRNA_TMM8_k017Y" which is a "Case" becomes "k017Y_Case"
# We need to keep the pairs of samples in the same training set
sampleInfo$sampleName <- paste(str_extract(
        sampleInfo$sample, "k.*"), "_",
        sampleInfo$condition, sep = "")

# Remove a few samples that would duplicate the new ames
sampleInfo = sampleInfo[!sampleInfo$sample %in%
                            c('ol_srna_tmm19_k668g2', 'ol_srna_tmm19_k837y2',
                              'ol_srna_tmm17_k604b2') , ]

cat("There are ", length(sampleInfo[sampleInfo$condition == 'Case', ]$sample), "Case samples")
cat("There are ", length(sampleInfo[sampleInfo$condition == 'Control', ]$sample), "Control samples")

```

Now, we remove from the matrix of counts all samples that are not in the sampleInfo file.  After that, we're left with 262 samples in the matrix of raw counts.


``` {r rename-samples-in-count-matrix}

# Keep the samples of the count matrix that are in the sample info file.
rawCountsId = rawCounts[names(rawCounts) %in% sampleInfo$sample]

# Identify the samples that are not paired.
idSamples <- colnames(rawCountsId)
allSamples <- colnames(rawCounts)
excludedSamples <- allSamples[!(allSamples %in% idSamples)]
cat("There are", length(excludedSamples), "samples excluded from the dataset")

# Rename columns of the count matrix.
tmp <- as.data.frame(colnames(rawCountsId))
tmp$new_name = apply(tmp, 1, function(x) sampleInfo[sampleInfo$sample == x, 'sampleName'])

colnames(rawCountsId) = tmp$new_name

rm(tmp)

```


### Filter the miRNAs by their coverage and prepare a normalized matrix

We remove miRNAs that do not show at least 50 counts in one sample. We also remove miRNAs where the count is above the 99% percentile after removing the poorly-expressed miRNAs.

That leaves us with 5k-30k miRNAs (down from 635k), depending on the parameters.


```{R filter-coverage}

# Parameters
minCount <- 100 
maxPerc <- 0.99

# # We start by making a copy of the dataframe
rawCountsIdCov <- rawCountsId

# We save the column names
columnNames <- colnames(rawCountsIdCov)

# Filter rows by the max of their counts per miRNA

rawCountsIdCov$maxRow = apply(rawCountsIdCov, 1, function(x) max(x))

# Summary of the max number 
summary(rawCountsIdCov$maxRow)

# Filter the matrix of counts out of miRNAs whose max is not at least minCount
rawCountsIdCov = rawCountsIdCov[rawCountsIdCov$maxRow >= minCount, ] 

# Summary of max counts per miRNA after removing the barely-expressed miRNAs
summary(rawCountsIdCov$maxRow)

# Identify the upper bound for the max number of counts
maxCount <- quantile(rawCountsIdCov$maxRow, maxPerc) 

# Filter the matrix of row counts of miRNAs whose max is less than maxCounts
rawCountsIdCov = rawCountsIdCov[rawCountsIdCov$maxRow <= maxCount, ] 

# Summary of max counts per miRNA after removing the over-expressed miRNAs
summary(rawCountsIdCov$maxRow)


```

We then normalize the matrix of counts per miRNA (between 0 and 1)

``` {R normalize}
#-------------------------------
# Prepare a normalized matrix (need to convert to numeric first)
countsNorm <- mutate_all(rawCountsIdCov, function(x) as.numeric(as.character(x)))

# We normalize each row (and we need to use the transpose function)
countsNorm = t(apply(
  countsNorm, 1, function(x) round(
                          (x-min(x))/(max(x)-min(x)), 
                          3)))

# Convert the "large matrix" into a dataframe
countsNorm = as.data.frame(countsNorm)

# Remove the column with the maxRow
rawCountsIdCov = rawCountsIdCov[ , columnNames]
countsNorm = countsNorm[ , columnNames]
  
# Add a +1 tous les cells to avoid an error with DESeq2
#rawCountsPairedCov = rawCountsPairedCov + 1


```


## Split the dataset into training (80%) and validation (20%) sets.


We randomized the dataset and split it into 2 training and test data sets to avoid over-fitting. The original dataset has 262 samples balanced between 108 Controls and 108 Cases.

When splitting the dataset, we make sure to keep the pairs of (Case, Control) in the same subset (training or test).

``` {R split-data}

# set seed to ensure reproducible results
set.seed(123)

# Randomize the list of samples
sampledNames <- sample(colnames(countsNorm))

# Take 20% of these sample positions at random for the test dataset
sampleIndices <- 1:length(sampledNames)
testIndices <- sample(sampleIndices, trunc(length(sampleIndices)/3))

testSamples <- sampledNames[testIndices]
trainSamples <- sampledNames[-testIndices]

# Create dataframes for testing and training
countsTest = rawCountsIdCov[grepl(paste(testSamples, collapse = "|"), colnames(rawCountsIdCov))]
countsTrain = rawCountsIdCov[grepl(paste(trainSamples, collapse = "|"), colnames(rawCountsIdCov))]

normTest = countsNorm[grepl(paste(testSamples, collapse = "|"), colnames(countsNorm))]
normTrain = countsNorm[grepl(paste(trainSamples, collapse = "|"), colnames(countsNorm))]


```


## Identify representative miRNAs using clustering techniques

Using boundaries on the number of counts we were able to decrease the number of miRNAs from 635k to 28k but there are still 100x more features than samples, which is termed the "curse of dimensionality".

### Method #1: T-test on each miRNA between the Cases and Controls

This method is the most intuitive: for each miRNA, we measure if there is a significant differently-expressed measurement between the Cases and the Controls.

``` {R t-test}

ttestDataset = countsTrain

# Lists of samples that are "Control"  and  "Case"
controlsTrain = ttestDataset[grepl("Control", colnames(ttestDataset))]
casesTrain = ttestDataset[grepl("Case", colnames(ttestDataset))]

# Create a column with the row names
ttestDataset$mirna = rownames(ttestDataset)

# Compute the  p-value of a t-test for each miRNA
ttestDataset$pValue <- apply(ttestDataset, 1,
                              function(x) round(wilcox.test(
                                as.numeric(controlsTrain[x['mirna'], ]),
                                as.numeric(casesTrain[x['mirna'], ]),
                                paired = FALSE,
                                alternative = "two.sided")$p.value, 3))

ttestDataset$controlMean <- apply(ttestDataset, 1,
                              function(x) round(mean( 
                                as.numeric( controlsTrain[x['mirna'], ] ), 
                                na.rm = FALSE ),3))

ttestDataset$caseMean <- apply(ttestDataset, 1,
                              function(x) round(mean( 
                                as.numeric(casesTrain[x['mirna'], ]), 
                                na.rm = FALSE ), 3))

ttestDataset$effectSize <- round(ttestDataset$caseMean / ttestDataset$controlMean, 3)
                              
# Correct for multiple testing using Benjamini & Hochberg 
# criteria (commented because not used)
ttestDataset$adjPValue <- p.adjust(ttestDataset$pValue, method = "BH")

# Delete intermediary files
rm(controlsTrain, casesTrain)

# Display statistics on the effect size
summary(ttestDataset$effectSize)

# Display statistics on the p-value
summary(ttestDataset$pValue)

#----------------------------------------------------
# Remove the miRNA where the effect size is NA
ttestDataset = ttestDataset[!is.na(ttestDataset$effectSize), ]

#----------------------------------------------------

# Pick min effect size (we picked it to obtain ~20 miRNAs)
minEffectSize = 3 

# Pick max p-value
maxPValue = 0.05

# Filter based on effect size
ttestDataset <- ttestDataset[ttestDataset$effectSize > minEffectSize |
                           ttestDataset$effectSize < 1/minEffectSize, ]

# Filter based on p-value
ttestDataset = ttestDataset[ttestDataset$pValue < maxPValue, ]

# Create an array of the selected miRNAs
ttestmiRNA <- rownames(ttestDataset)
cat("There are", length(ttestmiRNA), "miRNA selected by the t-test methodology")


```

### Method #2: DESeq2


```{R DESeq2-analysis}

# ## DESeq2 Analysis
# miR_dds <- DESeqDataSetFromMatrix(train_df, colData = sample_info_train, design = ~ condition)
# miR_dds$condition <- relevel(miR_dds$condition, ref = "Control")
# miR_dds <- DESeq(miR_dds)
# resultsNames(miR_dds) # list the coefficients
# 
# ## DESeq2 results
# miR_res <- results(miR_dds, filterFun = ihw, alpha = 0.05, name = "condition_Case_vs_Control")
# summary(miR_res)
# plotMA( miR_res, ylim = c(-1, 1) )
# 
# miR_res_df <- as.data.frame(miR_res)
# ## Function to grab results
# get_upregulated <- function(df){
#     key <- intersect(rownames(df)[which(df$log2FoldChange>=1)],
#               rownames(df)[which(df$pvalue<=0.05)])
#     
#     results <- as.data.frame((df)[which(rownames(df) %in% key),])
#     return(results)
#   }
# get_downregulated <- function(df){
#   key <- intersect(rownames(df)[which(df$log2FoldChange<=-1)],
#             rownames(df)[which(df$pvalue<=0.05)])
#   
#   results <- as.data.frame((df)[which(rownames(df) %in% key),])
#   return(results)
# }
# miR_upreg <- get_upregulated(miR_res)
# miR_downreg <- get_downregulated(miR_res)
# ## Write results for plots and analysis
# miR_counts <- counts(miR_dds, normalized = T)
# 
# # Create a directory where to write the results
# dir.create("results/")
# 
# # Write the results
# write.table(miR_counts, "results/miR_norm.counts.txt", quote = F, sep = "\t")
# miR_upreg$miRNA_id <- rownames(miR_upreg)
# miR_downreg$miRNA_id <- rownames(miR_downreg)
# miR_upreg <- miR_upreg[,c(8,1,2,3,4,5,6,7)]
# miR_downreg <- miR_downreg[,c(8,1,2,3,4,5,6,7)]
# write.table(miR_upreg, "results/miR_upreg.txt", quote = F, sep = "\t", row.names = F)
# write.table(miR_downreg, "results/miR_downreg.txt", quote = F, sep = "\t", row.names = F)

```


### Filter datasets with the selected miRNAs

``` {R filter datasets}

mirnaModel <- ttestmiRNA

# Matrices of counts
countsTrainFilt = countsTrain[mirnaModel, ]
countsTestFilt = countsTest[mirnaModel, ]

# Matrices of normalized counts
normTestFilt <- normTest[mirnaModel, ]
normTrainFilt <- normTrain[mirnaModel, ]


```


## Predictions using machine-learning techniques

### Variables used in the ML models

``` {r variables}
conditionBinary = ifelse(testSet$condition == "Control", 0, 1)

```



### Visualize the data using multi-dimensional scaling on the training dataset

``` {R visualize-results}

# Remove the calculation variables from normTrainFilt 
trainingSet = normTrainFilt[, !names(normTrainFilt) %in% 
                   c('mirna', 'pValue', 
                     'caseMean', 'controlMean', 'effectSize', 'adjPValue')]

# Transpose the dataset to have rows as samples and columns as miRNAs (features)
trainingSet <- as.data.frame(t(trainingSet))
testSet <- as.data.frame(t(normTestFilt))

# Grab the names of the miRNAs
mirna <- colnames(trainingSet)

# Gather names of columns before manipulation
trainingSamples <- colnames(trainingSet)

# Create a column with the samples names
trainingSet$sample = rownames(trainingSet)
testSet$sample = rownames(testSet)

# Function to figure out if the sample is a "Control" or a "Sample"
findCondition <- function(x) {
  if ( grepl('Case', x) ) {
    answer = "Case"
  } else {
      answer = "Control"
  } 
  return (answer)}

# Apply function to create a new column
trainingSet$condition = apply(trainingSet['sample'], 1, findCondition)
testSet$condition = apply(testSet['sample'], 1, findCondition)


# New column names (re-ordered)
trainingSet = trainingSet[, c('condition', mirna)]
testSet = testSet[, c('condition', mirna)]
#test_scaled_df = test_scaled_df[, !colnames(test_scaled_df) %in% 'sample']

# Check the data type of the data frame
# sapply(for_model_df_t, class) 

# Create MDS dataset in 2 dimensions
mds <- trainingSet %>%
  dist() %>%
  cmdscale() %>%
  as_tibble()
colnames(mds) <- c("Dim.1", "Dim.2")
# Add the condition to the dataframe
mds$condition <- trainingSet$condition
mds$condition = as.factor(mds$condition)

# Plot MDS for all data
p <- ggscatter(mds, x = "Dim.1", y = "Dim.2",
        size = 2,
        alpha = 0.5,
        color = 'condition',
        palette =  c("#00AFBB", "#FC4E07"),
        repel = TRUE
        )

print(p)


```



### Classification using lasso logistic regression

Our goal is to select as few variables as possible (because of experimental constraints). Therefore, we run a penalized logistic regression using the lasso regression. In this regression, the coefficients of some less contributive variables are forced to be exactly zero. Only the most significant variables are kept in the final model. The


``` {R lasso}

# Dumy code categorical predictor variables
xTraining <- model.matrix(condition~., trainingSet)[,-1]
# Convert the outcome (class) to a numerical variable
yTraining <- ifelse(trainingSet$condition == "Case", 1, 0)

# Model
cvLasso <- cv.glmnet(xTraining, yTraining, alpha = 1, family = "binomial")

# Display binomial deviance
#plot(cvLasso)

# Build model with lamnda min
lassoModel <- glmnet(xTraining, yTraining, alpha = 1, family = "binomial", lambda = cvLasso$lambda.min)


xTest <- model.matrix(condition ~., testSet)[,-1]
probabilities <- lassoModel %>% predict(newx = xTest, type="response")
predictedClasses <- ifelse(probabilities > 0.5, "Case", "Control")

# Confusion matrix
table(pred = predictedClasses, true = testSet[, c('condition')])

# Model accuracy
modelAccuracy = mean(predictedClasses == testSet$condition)
cat("The accuracy is", modelAccuracy, "\n")

# ROC curve
pred <- prediction(as.vector(probabilities), as.vector(conditionBinary))
perf <- performance(pred,"tpr","fpr")
par(pty="s")


# Plot the ROC curve
plot(perf,  main = "ROC curve")
# plot the no-prediction line
lines(c(0,1),c(0,1),col = "gray", lty = 4 )
auc_ROCR <- performance(pred, measure = "auc")
  auc_ROCR <- auc_ROCR@y.values[[1]]

cat("The AUC is", auc_ROCR)  

# Regression parameters  
#coef(cv_lasso, cv_lasso$lambda.min)


# precision/recall curve (x-axis: recall, y-axis: precision)
perf <- performance(pred, "prec", "rec")
plot(perf)

# sensitivity/specificity curve (x-axis: specificity,
# y-axis: sensitivity)
perf <- performance(pred, "sens", "spec")
plot(perf)

# Coefficients
coef(cvLasso, cvLasso$lambda.min)
```

### Classification using support vector machines

``` {R svm}

set.seed(42)

# Treat the condition as a factor
trainingSet$condition = as.factor(trainingSet$condition)
svmModel <- svm(condition ~ ., data = trainingSet, kernel = "polynomial", cost = 100, gamma = 10, probability = TRUE)
svmPred <- predict(svmModel, 
                    testSet[ , !names(testSet) %in% c('condition')], probability = TRUE)

# Confusion matrix
table(pred = svmPred, true = testSet[, c('condition')])

# Model accuracy
svmAccuracy = mean(svmPred == testSet$condition)
cat("The accuracy is", svmAccuracy, "\n")

# ROC
pred <- prediction(as.data.frame(attr(svmPred, "probabilities"))$Case, conditionBinary)
perf <- performance(pred,"tpr","fpr")
par(pty="s")

# Plot the ROC curve
plot(perf,  main = "ROC curve")
# plot the no-prediction line
lines(c(0,1),c(0,1),col = "gray", lty = 4 )
svmAUC <- performance(pred, measure = "auc")
svmAUC <- svmAUC@y.values[[1]]

cat("The AUC is", svmAUC)  

# Regression parameters  
#coef(cv_lasso, cv_lasso$lambda.min)


# precision/recall curve (x-axis: recall, y-axis: precision)
perf <- performance(pred, "prec", "rec")
plot(perf)

# sensitivity/specificity curve (x-axis: specificity,
# y-axis: sensitivity)
perf <- performance(pred, "sens", "spec")
plot(perf)

```


### Classification using regression trees

``` {R regression-tree}

## Regression tree
rpartModel <- rpart(condition ~ ., data = testSet)
rpartPred <- predict(rpartModel, testSet[ , !names(testSet) 
                              %in% c('condition')], type = "class")

# Confusion matrix for rpart
table(pred = rpartPred, true = testSet[, c('condition')])

# Accuracy
rpartAccuracy = mean(rpartPred == testSet$condition)
cat("The accuracy is", rpartAccuracy, "\n")

# ROC
pred <- prediction(as.data.frame(attr(rpartPred, "probabilities"))$Case, conditionBinary)
perf <- performance(pred,"tpr","fpr")
par(pty="s")

# Plot the ROC curve
plot(perf,  main = "ROC curve")
# plot the no-prediction line
lines(c(0,1),c(0,1),col = "gray", lty = 4 )
rpartAUC <- performance(pred, measure = "auc")
rpartAUC <- rpartAUC@y.values[[1]]

cat("The AUC is", svmAUC)  

# Regression parameters  
#coef(cv_lasso, cv_lasso$lambda.min)


# precision/recall curve (x-axis: recall, y-axis: precision)
perf <- performance(pred, "prec", "rec")
plot(perf)

# sensitivity/specificity curve (x-axis: specificity,
# y-axis: sensitivity)
perf <- performance(pred, "sens", "spec")
plot(perf)


```

